#!/usr/bin/env python3
"""
HVDC ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ ì™„ì „ ìë™í™” ë¶„ë¥˜ ì‹œìŠ¤í…œ
==========================================

ì˜¨í†¨ë¡œì§€ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ì •ì˜ëœ ê·¸ë£¹ ê¸°ì¤€ìœ¼ë¡œë§Œ ë¶„ë¥˜í•˜ì—¬
íŒ¨í„´ ë§¤ì¹­, ëŒ€ì†Œë¬¸ì, ì˜¤íƒ€ ë“±ì˜ ì˜¤ë¥˜ë¥¼ ì™„ì „íˆ ì°¨ë‹¨í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    from hvdc_ontology_based_classifier import get_location_group, create_ontology_warehouse_flow
    
    df['LocationGroup'] = df['hasLocation'].apply(get_location_group)
    warehouse_flow = create_ontology_warehouse_flow(df)
"""

import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import json

# ===============================================================================
# 1. ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ ì™„ì „ ìë™í™” ë¶„ë¥˜ ë”•ì…”ë„ˆë¦¬
# ===============================================================================

# ì˜¨í†¨ë¡œì§€ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ì •ì˜ëœ ê·¸ë£¹
INDOOR_WAREHOUSE = ["DSV Indoor", "DSV Al Markaz", "Hauler Indoor"]
OUTDOOR_WAREHOUSE = ["DSV Outdoor", "DSV MZP", "MOSB"]
SITE = ["AGI", "DAS", "MIR", "SHU"]
DANGEROUS_CARGO = ["AAA Storage", "Dangerous Storage"]

def get_location_group(name):
    """
    ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ ìœ„ì¹˜ ê·¸ë£¹ ë¶„ë¥˜ (100% ëª…ì‹œì  ë§¤ì¹­)
    
    Args:
        name: ìœ„ì¹˜ëª… (ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ê¸°ì¤€)
        
    Returns:
        str: 'IndoorWarehouse', 'OutdoorWarehouse', 'Site', 'DangerousCargoWarehouse', 'UNKNOWN'
    """
    if pd.isna(name):
        return "UNKNOWN"
    
    # ì •í™•í•œ ë¬¸ìì—´ ë§¤ì¹­ë§Œ ìˆ˜í–‰ (íŒ¨í„´ ë§¤ì¹­ ê¸ˆì§€)
    n = str(name).strip()
    
    if n in INDOOR_WAREHOUSE:
        return "IndoorWarehouse"
    elif n in OUTDOOR_WAREHOUSE:
        return "OutdoorWarehouse"
    elif n in SITE:
        return "Site"
    elif n in DANGEROUS_CARGO:
        return "DangerousCargoWarehouse"
    else:
        return "UNKNOWN"

def get_warehouse_locations():
    """ëª¨ë“  ì°½ê³  ìœ„ì¹˜ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    return INDOOR_WAREHOUSE + OUTDOOR_WAREHOUSE + DANGEROUS_CARGO

def get_site_locations():
    """ëª¨ë“  í˜„ì¥ ìœ„ì¹˜ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    return SITE

def get_all_known_locations():
    """ëª¨ë“  ì•Œë ¤ì§„ ìœ„ì¹˜ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    return INDOOR_WAREHOUSE + OUTDOOR_WAREHOUSE + SITE + DANGEROUS_CARGO

def validate_location_data(df, location_column='hasSite'):
    """
    ìœ„ì¹˜ ë°ì´í„° ê²€ì¦ ë° ë¶„ì„
    
    Args:
        df: DataFrame
        location_column: ìœ„ì¹˜ ì»¬ëŸ¼ëª…
        
    Returns:
        dict: ê²€ì¦ ê²°ê³¼
    """
    print("ğŸ” ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ ìœ„ì¹˜ ë°ì´í„° ê²€ì¦ ì¤‘...")
    
    if location_column not in df.columns:
        return {"error": f"ì»¬ëŸ¼ '{location_column}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}
    
    # ìœ„ì¹˜ ê·¸ë£¹ ë¶„ë¥˜
    df_temp = df.copy()
    df_temp['LocationGroup'] = df_temp[location_column].apply(get_location_group)
    
    # ë¶„ì„ ê²°ê³¼
    location_counts = df_temp[location_column].value_counts()
    group_counts = df_temp['LocationGroup'].value_counts()
    
    # ì•Œë ¤ì§€ì§€ ì•Šì€ ìœ„ì¹˜ë“¤
    unknown_locations = df_temp[df_temp['LocationGroup'] == 'UNKNOWN'][location_column].value_counts()
    
    result = {
        "total_records": len(df_temp),
        "unique_locations": len(location_counts),
        "location_distribution": location_counts.to_dict(),
        "group_distribution": group_counts.to_dict(),
        "unknown_locations": unknown_locations.to_dict(),
        "validation_summary": {
            "indoor_warehouse": len(df_temp[df_temp['LocationGroup'] == 'IndoorWarehouse']),
            "outdoor_warehouse": len(df_temp[df_temp['LocationGroup'] == 'OutdoorWarehouse']),
            "site": len(df_temp[df_temp['LocationGroup'] == 'Site']),
            "dangerous_cargo": len(df_temp[df_temp['LocationGroup'] == 'DangerousCargoWarehouse']),
            "unknown": len(df_temp[df_temp['LocationGroup'] == 'UNKNOWN'])
        }
    }
    
    print("âœ… ìœ„ì¹˜ ë°ì´í„° ê²€ì¦ ì™„ë£Œ")
    print(f"ğŸ“Š ê·¸ë£¹ë³„ ë¶„í¬: {result['group_distribution']}")
    
    if unknown_locations.any():
        print(f"âš ï¸ ì•Œë ¤ì§€ì§€ ì•Šì€ ìœ„ì¹˜ {len(unknown_locations)}ê°œ ë°œê²¬:")
        print(unknown_locations.head())
    
    return result

# ===============================================================================
# 2. ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ íŠ¸ëœì­ì…˜ íƒ€ì… ë¶„ë¥˜
# ===============================================================================

def classify_transaction_type_ontology(row):
    """
    ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ íŠ¸ëœì­ì…˜ íƒ€ì… ë¶„ë¥˜
    
    Args:
        row: DataFrameì˜ í–‰ ë°ì´í„°
        
    Returns:
        str: 'IN', 'OUT', 'TRANSFER', 'UNKNOWN'
    """
    # hasTransactionType ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš° ìš°ì„  ì‚¬ìš©
    if 'hasTransactionType' in row and pd.notna(row['hasTransactionType']):
        tx_type = str(row['hasTransactionType']).upper().strip()
        if tx_type == 'IN' or 'INBOUND' in tx_type:
            return 'IN'
        elif tx_type == 'OUT' or 'OUTBOUND' in tx_type:
            return 'OUT'
        elif 'TRANSFER' in tx_type:
            return 'TRANSFER'
    
    # hasCurrentStatus ê¸°ë°˜ ë¶„ë¥˜
    if 'hasCurrentStatus' in row and pd.notna(row['hasCurrentStatus']):
        status = str(row['hasCurrentStatus']).upper().strip()
        if any(keyword in status for keyword in ['IN', 'RECEIVE', 'ARRIVAL', 'INBOUND']):
            return 'IN'
        elif any(keyword in status for keyword in ['OUT', 'SHIP', 'DELIVERY', 'OUTBOUND']):
            return 'OUT'
        elif 'TRANSFER' in status:
            return 'TRANSFER'
    
    # ìˆ˜ëŸ‰ ê¸°ë°˜ ì¶”ì • (ì–‘ìˆ˜=ì…ê³ , ìŒìˆ˜=ì¶œê³ )
    quantity_cols = ['hasQuantity', 'hasVolume_numeric', 'hasVolume']
    for col in quantity_cols:
        if col in row and pd.notna(row[col]):
            try:
                qty = float(row[col])
                if qty > 0:
                    return 'IN'
                elif qty < 0:
                    return 'OUT'
            except (ValueError, TypeError):
                continue
    
    # ê¸°ë³¸ê°’: ì…ê³ ë¡œ ê°€ì •
    return 'IN'

# ===============================================================================
# 3. ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ ì°½ê³  íë¦„ ë¶„ì„ í•¨ìˆ˜
# ===============================================================================

def create_ontology_warehouse_flow(df, location_column='hasSite'):
    """ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ ì°½ê³ ë³„ ì›”ë³„ ì…ê³ /ì¶œê³ /ì¬ê³  íë¦„ ë¶„ì„"""
    print("ğŸ”„ ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ ì°½ê³ ë³„ ì›”ë³„ ì…ì¶œê³  íë¦„ ë¶„ì„ ì‹œì‘...")
    
    df_work = df.copy()
    
    # 1. ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ ìœ„ì¹˜ ê·¸ë£¹ ë¶„ë¥˜
    df_work['LocationGroup'] = df_work[location_column].apply(get_location_group)
    
    # 2. ì°½ê³ ë§Œ í•„í„°ë§ (í˜„ì¥ ì œì™¸)
    warehouse_groups = ['IndoorWarehouse', 'OutdoorWarehouse', 'DangerousCargoWarehouse']
    warehouse_df = df_work[df_work['LocationGroup'].isin(warehouse_groups)].copy()
    
    if warehouse_df.empty:
        print("âš ï¸ ì°½ê³  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()
    
    print(f"ğŸ“Š ì°½ê³  ë°ì´í„° í•„í„°ë§ ê²°ê³¼: {len(warehouse_df):,}ê°œ ë ˆì½”ë“œ")
    
    # 3. ì›”ë³„ ì»¬ëŸ¼ ìƒì„±
    warehouse_df['Month'] = pd.to_datetime(warehouse_df['hasDate']).dt.to_period("M")
    all_months = pd.period_range(warehouse_df['Month'].min(), warehouse_df['Month'].max(), freq='M')
    
    print(f"ğŸ“… ë¶„ì„ ê¸°ê°„: {all_months[0]} ~ {all_months[-1]} ({len(all_months)}ê°œì›”)")
    
    # 4. ì…ê³ /ì¶œê³  ìˆ˜ëŸ‰ ë¶„ë¦¬ (ëª¨ë“  ë°ì´í„°ë¥¼ ì…ê³ ë¡œ ê°€ì •)
    quantity_col = 'hasVolume_numeric' if 'hasVolume_numeric' in warehouse_df.columns else 'hasVolume'
    if quantity_col not in warehouse_df.columns:
        warehouse_df[quantity_col] = 0
    
    warehouse_df['InQty'] = warehouse_df[quantity_col]
    warehouse_df['OutQty'] = 0  # ì¶œê³  ë°ì´í„°ê°€ ë³„ë„ë¡œ ì—†ìœ¼ë¯€ë¡œ 0ìœ¼ë¡œ ì„¤ì •
    warehouse_df['TransferQty'] = 0
    
    # 5. ì›”ë³„ ì§‘ê³„
    amount_col = 'hasAmount_numeric' if 'hasAmount_numeric' in warehouse_df.columns else 'hasAmount'
    if amount_col not in warehouse_df.columns:
        warehouse_df[amount_col] = 0
    
    monthly_flow = warehouse_df.groupby([location_column, 'LocationGroup', 'Month']).agg({
        'InQty': 'sum',
        'OutQty': 'sum', 
        'TransferQty': 'sum',
        amount_col: 'sum'
    }).round(2)
    
    # 6. ì¬ê³  ê³„ì‚° (ëˆ„ì  ì…ê³ )
    monthly_flow['Net_Flow'] = monthly_flow['InQty'] - monthly_flow['OutQty'] + monthly_flow['TransferQty']
    monthly_flow['Cumulative_Stock'] = monthly_flow.groupby(level=[0, 1])['Net_Flow'].cumsum()
    
    # 7. ì „ì²´ ì›” ë²”ìœ„ë¡œ reindex
    warehouse_list = monthly_flow.index.get_level_values(0).unique()
    group_list = monthly_flow.index.get_level_values(1).unique()
    
    # ìœ„ì¹˜ë³„ ê·¸ë£¹ ë§¤í•‘ ìƒì„±
    location_group_map = warehouse_df.drop_duplicates([location_column, 'LocationGroup']).set_index(location_column)['LocationGroup'].to_dict()
    
    multi_index_data = []
    for location in warehouse_list:
        group = location_group_map[location]
        for month in all_months:
            multi_index_data.append((location, group, month))
    
    multi_index = pd.MultiIndex.from_tuples(
        multi_index_data, 
        names=[location_column, 'LocationGroup', 'Month']
    )
    
    monthly_flow = monthly_flow.reindex(multi_index, fill_value=0)
    
    # 8. ìµœì¢… í¬ë§·íŒ…
    result = monthly_flow.reset_index()
    result.columns = ['ìœ„ì¹˜ëª…', 'ìœ„ì¹˜ê·¸ë£¹', 'ì›”', 'ì…ê³ ìˆ˜ëŸ‰', 'ì¶œê³ ìˆ˜ëŸ‰', 'ì´ë™ìˆ˜ëŸ‰', 'ê¸ˆì•¡', 'ìˆœì¦ê°', 'ëˆ„ì ì¬ê³ ']
    
    print(f"âœ… ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ ì°½ê³ ë³„ íë¦„ ë¶„ì„ ì™„ë£Œ: {len(warehouse_list)}ê°œ ì°½ê³ , {len(all_months)}ê°œì›”")
    
    return result

def create_ontology_site_delivery(df, location_column='hasSite'):
    """ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ í˜„ì¥ë³„ ë°°ì†¡ í˜„í™© ë¶„ì„"""
    print("ğŸ”„ ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ í˜„ì¥ë³„ ë°°ì†¡ í˜„í™© ë¶„ì„ ì‹œì‘...")
    
    df_work = df.copy()
    
    # 1. ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ ìœ„ì¹˜ ê·¸ë£¹ ë¶„ë¥˜
    df_work['LocationGroup'] = df_work[location_column].apply(get_location_group)
    
    # 2. í˜„ì¥ë§Œ í•„í„°ë§
    site_df = df_work[df_work['LocationGroup'] == 'Site'].copy()
    
    if site_df.empty:
        print("âš ï¸ í˜„ì¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()
    
    print(f"ğŸ“Š í˜„ì¥ ë°ì´í„° í•„í„°ë§ ê²°ê³¼: {len(site_df):,}ê°œ ë ˆì½”ë“œ")
    
    # 3. ì›”ë³„ ì§‘ê³„
    site_df['Month'] = pd.to_datetime(site_df['hasDate']).dt.to_period("M")
    all_months = pd.period_range(site_df['Month'].min(), site_df['Month'].max(), freq='M')
    
    quantity_col = 'hasVolume_numeric' if 'hasVolume_numeric' in site_df.columns else 'hasVolume'
    amount_col = 'hasAmount_numeric' if 'hasAmount_numeric' in site_df.columns else 'hasAmount'
    
    if quantity_col not in site_df.columns:
        site_df[quantity_col] = 0
    if amount_col not in site_df.columns:
        site_df[amount_col] = 0
    
    site_delivery = site_df.groupby([location_column, 'Month']).agg({
        quantity_col: ['sum', 'count'],
        amount_col: 'sum'
    }).round(2)
    
    # ì»¬ëŸ¼ëª… ì •ë¦¬
    site_delivery.columns = ['ë°°ì†¡ìˆ˜ëŸ‰', 'ë°°ì†¡íšŸìˆ˜', 'ë°°ì†¡ê¸ˆì•¡']
    
    # 4. ì „ì²´ ì›” ë²”ìœ„ë¡œ reindex
    site_list = site_delivery.index.get_level_values(0).unique()
    multi_index = pd.MultiIndex.from_product(
        [site_list, all_months], 
        names=[location_column, 'Month']
    )
    
    site_delivery = site_delivery.reindex(multi_index, fill_value=0)
    
    # 5. ìµœì¢… í¬ë§·íŒ…
    result = site_delivery.reset_index()
    result.columns = ['í˜„ì¥ëª…', 'ì›”', 'ë°°ì†¡ìˆ˜ëŸ‰', 'ë°°ì†¡íšŸìˆ˜', 'ë°°ì†¡ê¸ˆì•¡']
    
    print(f"âœ… ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ í˜„ì¥ë³„ ë°°ì†¡ ë¶„ì„ ì™„ë£Œ: {len(site_list)}ê°œ í˜„ì¥, {len(all_months)}ê°œì›”")
    
    return result

# ===============================================================================
# 4. ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ í†µí•© ë¦¬í¬íŠ¸ ìƒì„±
# ===============================================================================

def create_ontology_based_reports(df, location_column='hasSite', date_column='hasDate'):
    """
    ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ í†µí•© ë¦¬í¬íŠ¸ ìƒì„±
    
    Args:
        df: ì›ë³¸ DataFrame
        location_column: ìœ„ì¹˜ ì»¬ëŸ¼ëª…
        date_column: ë‚ ì§œ ì»¬ëŸ¼ëª…
        
    Returns:
        Dict[str, pd.DataFrame]: ë¦¬í¬íŠ¸ë³„ DataFrame ë”•ì…”ë„ˆë¦¬
    """
    print("ğŸš€ ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")
    print("=" * 80)
    
    # 1. ìœ„ì¹˜ ë°ì´í„° ê²€ì¦
    validation_result = validate_location_data(df, location_column)
    
    reports = {}
    
    # 2. ì°½ê³ ë³„ ì›”ë³„ ì…ì¶œê³  íë¦„
    warehouse_flow = create_ontology_warehouse_flow(df, location_column)
    if not warehouse_flow.empty:
        reports['ì°½ê³ ë³„_ì›”ë³„_ì…ì¶œê³ ì¬ê³ '] = warehouse_flow
    
    # 3. í˜„ì¥ë³„ ë°°ì†¡ í˜„í™©
    site_delivery = create_ontology_site_delivery(df, location_column)
    if not site_delivery.empty:
        reports['í˜„ì¥ë³„_ë°°ì†¡í˜„í™©'] = site_delivery
    
    # 4. ìœ„ì¹˜ ê·¸ë£¹ë³„ ìš”ì•½ í†µê³„
    if not warehouse_flow.empty:
        warehouse_summary = warehouse_flow.groupby(['ìœ„ì¹˜ëª…', 'ìœ„ì¹˜ê·¸ë£¹']).agg({
            'ì…ê³ ìˆ˜ëŸ‰': 'sum',
            'ì¶œê³ ìˆ˜ëŸ‰': 'sum',
            'ì´ë™ìˆ˜ëŸ‰': 'sum',
            'ê¸ˆì•¡': 'sum',
            'ëˆ„ì ì¬ê³ ': 'last'
        }).round(2)
        warehouse_summary.columns = ['ì´ì…ê³ ', 'ì´ì¶œê³ ', 'ì´ì´ë™', 'ì´ê¸ˆì•¡', 'í˜„ì¬ì¬ê³ ']
        reports['ì°½ê³ ë³„_ìš”ì•½í†µê³„'] = warehouse_summary.reset_index()
    
    if not site_delivery.empty:
        site_summary = site_delivery.groupby('í˜„ì¥ëª…').agg({
            'ë°°ì†¡ìˆ˜ëŸ‰': 'sum',
            'ë°°ì†¡íšŸìˆ˜': 'sum',
            'ë°°ì†¡ê¸ˆì•¡': 'sum'
        }).round(2)
        site_summary.columns = ['ì´ë°°ì†¡ìˆ˜ëŸ‰', 'ì´ë°°ì†¡íšŸìˆ˜', 'ì´ë°°ì†¡ê¸ˆì•¡']
        reports['í˜„ì¥ë³„_ìš”ì•½í†µê³„'] = site_summary.reset_index()
    
    # 5. ì˜¨í†¨ë¡œì§€ ë¶„ë¥˜ ê²°ê³¼
    reports['ì˜¨í†¨ë¡œì§€_ë¶„ë¥˜ê²°ê³¼'] = pd.DataFrame([
        {"ë¶„ë¥˜": "Indoor Warehouse", "ìœ„ì¹˜": ", ".join(INDOOR_WAREHOUSE)},
        {"ë¶„ë¥˜": "Outdoor Warehouse", "ìœ„ì¹˜": ", ".join(OUTDOOR_WAREHOUSE)},
        {"ë¶„ë¥˜": "Site", "ìœ„ì¹˜": ", ".join(SITE)},
        {"ë¶„ë¥˜": "Dangerous Cargo", "ìœ„ì¹˜": ", ".join(DANGEROUS_CARGO)}
    ])
    
    print("=" * 80)
    print("ğŸ‰ ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“Š ìƒì„±ëœ ë¦¬í¬íŠ¸: {len(reports)}ê°œ")
    for report_name, report_df in reports.items():
        print(f"   â€¢ {report_name}: {report_df.shape}")
    
    return reports

def save_ontology_reports_excel(reports, output_path=None):
    """
    ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ ë¦¬í¬íŠ¸ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥
    
    Args:
        reports: ë¦¬í¬íŠ¸ ë”•ì…”ë„ˆë¦¬
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        
    Returns:
        str: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    if output_path is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M')
        output_path = f"HVDC_ì˜¨í†¨ë¡œì§€ê¸°ì¤€_ì°½ê³ íë¦„ë¶„ì„_{timestamp}.xlsx"
    
    print(f"ğŸ’¾ ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ Excel ë¦¬í¬íŠ¸ ì €ì¥ ì¤‘: {output_path}")
    
    with pd.ExcelWriter(output_path, engine="xlsxwriter") as writer:
        workbook = writer.book
        
        # ê° ë¦¬í¬íŠ¸ë¥¼ ì‹œíŠ¸ë¡œ ì €ì¥
        for sheet_name, report_df in reports.items():
            # ì‹œíŠ¸ëª… ê¸¸ì´ ì œí•œ (Excel 31ì ì œí•œ)
            safe_sheet_name = sheet_name[:31] if len(sheet_name) > 31 else sheet_name
            
            report_df.to_excel(writer, sheet_name=safe_sheet_name, index=False)
            
            # ì¡°ê±´ë¶€ ì„œì‹ ì ìš©
            worksheet = writer.sheets[safe_sheet_name]
            try:
                if len(report_df) > 0:
                    # ìˆ«ì ì»¬ëŸ¼ì—ë§Œ ì¡°ê±´ë¶€ ì„œì‹ ì ìš©
                    numeric_cols = report_df.select_dtypes(include=[np.number]).columns
                    if len(numeric_cols) > 0:
                        worksheet.conditional_format(1, 1, len(report_df), len(report_df.columns)-1, {
                            "type": "3_color_scale",
                            "min_color": "#F8696B",
                            "mid_color": "#FFEB84", 
                            "max_color": "#63BE7B",
                        })
            except Exception as e:
                print(f"âš ï¸ ì¡°ê±´ë¶€ ì„œì‹ ì ìš© ì‹¤íŒ¨ ({safe_sheet_name}): {e}")
    
    print(f"âœ… ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ Excel ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {output_path}")
    return output_path

# ===============================================================================
# 5. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ===============================================================================

def run_ontology_based_analysis(excel_path=None, mapping_rules_path=None, location_column='hasSite', date_column='hasDate'):
    """
    ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ ì°½ê³  íë¦„ ë¶„ì„ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    
    Args:
        excel_path: Excel íŒŒì¼ ê²½ë¡œ
        mapping_rules_path: ë§¤í•‘ ê·œì¹™ íŒŒì¼ ê²½ë¡œ
        location_column: ìœ„ì¹˜ ì»¬ëŸ¼ëª…
        date_column: ë‚ ì§œ ì»¬ëŸ¼ëª…
        
    Returns:
        Dict[str, pd.DataFrame]: ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸
    """
    print("ğŸ¯ ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ HVDC ì°½ê³  íë¦„ ë¶„ì„ ì‹œì‘")
    print("=" * 80)
    
    start_time = datetime.now()
    
    try:
        # 1. ë°ì´í„° ë¡œë“œ
        if excel_path is None:
            excel_path = "data/HVDC WAREHOUSE_HITACHI(HE).xlsx"
        
        print(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘: {excel_path}")
        
        if mapping_rules_path:
            # ë§¤í•‘ ê·œì¹™ì´ ìˆëŠ” ê²½ìš°
            with open(mapping_rules_path, encoding='utf-8') as f:
                mapping_rules = json.load(f)['field_map']
            
            df_raw = pd.read_excel(excel_path)
            col_map = {k: v for k, v in mapping_rules.items() if k in df_raw.columns}
            df = df_raw.rename(columns=col_map)
            
            # í•„ìš” ì»¬ëŸ¼ ì¶”ê°€
            for needed in mapping_rules.values():
                if needed not in df.columns:
                    df[needed] = 0
        else:
            # ë§¤í•‘ ê·œì¹™ì´ ì—†ëŠ” ê²½ìš° ì›ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            df = pd.read_excel(excel_path)
        
        # ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬
        if date_column not in df.columns:
            date_candidates = ['ETD/ATD', 'ETA/ATA', 'hasDate']
            for candidate in date_candidates:
                if candidate in df.columns:
                    df[date_column] = pd.to_datetime(df[candidate], errors='coerce')
                    break
            else:
                df[date_column] = pd.Timestamp.now()
        
        df[date_column] = df[date_column].fillna(pd.Timestamp.now())
        
        # ìˆ˜ì¹˜ ì»¬ëŸ¼ ì²˜ë¦¬
        numeric_cols = ['hasAmount', 'hasVolume']
        for col in numeric_cols:
            if col in df.columns:
                df[f'{col}_numeric'] = pd.to_numeric(df[col], errors='coerce').fillna(0)
        
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape[0]:,}ê°œ ë ˆì½”ë“œ, {df.shape[1]}ê°œ ì»¬ëŸ¼")
        
        # 2. ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ ë¶„ì„ ì‹¤í–‰
        reports = create_ontology_based_reports(df, location_column, date_column)
        
        # 3. Excel ì €ì¥
        excel_output_path = save_ontology_reports_excel(reports)
        reports['_excel_path'] = excel_output_path
        
        # 4. ê²°ê³¼ ìš”ì•½
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        print("\n" + "=" * 80)
        print("ğŸ‰ ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ HVDC ì°½ê³  íë¦„ ë¶„ì„ ì™„ë£Œ!")
        print("=" * 80)
        
        print(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
        print(f"   â€¢ ì›ë³¸ ë°ì´í„°: {df.shape[0]:,}ê°œ ë ˆì½”ë“œ")
        print(f"   â€¢ ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
        print(f"   â€¢ Excel íŒŒì¼: {excel_output_path}")
        
        if 'ì°½ê³ ë³„_ìš”ì•½í†µê³„' in reports:
            print(f"   â€¢ ì°½ê³  ê°œìˆ˜: {len(reports['ì°½ê³ ë³„_ìš”ì•½í†µê³„'])}ê°œ")
        if 'í˜„ì¥ë³„_ìš”ì•½í†µê³„' in reports:
            print(f"   â€¢ í˜„ì¥ ê°œìˆ˜: {len(reports['í˜„ì¥ë³„_ìš”ì•½í†µê³„'])}ê°œ")
        
        return reports
        
    except Exception as e:
        print(f"âŒ ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ ë¶„ì„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return {}

# ===============================================================================
# 6. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ë¶€
# ===============================================================================

if __name__ == "__main__":
    """ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ ë¶„ë¥˜ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    test_locations = [
        "DSV Indoor", "DSV Outdoor", "DSV Al Markaz", "MOSB",  # ì°½ê³ 
        "AGI", "DAS", "MIR", "SHU",  # í˜„ì¥
        "AAA Storage", "Dangerous Storage",  # ìœ„í—˜ë¬¼ ì°½ê³ 
        "Unknown Location", "", None  # ì•Œë ¤ì§€ì§€ ì•Šì€ ìœ„ì¹˜
    ]
    
    print("ğŸ“‹ ìœ„ì¹˜ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸:")
    for location in test_locations:
        group = get_location_group(location)
        print(f"  {str(location):20} â†’ {group}")
    
    print(f"\nğŸ“Š ì˜¨í†¨ë¡œì§€ ì •ì˜:")
    print(f"  Indoor Warehouse: {INDOOR_WAREHOUSE}")
    print(f"  Outdoor Warehouse: {OUTDOOR_WAREHOUSE}")
    print(f"  Site: {SITE}")
    print(f"  Dangerous Cargo: {DANGEROUS_CARGO}")
    
    print(f"\nâœ… ì˜¨í†¨ë¡œì§€ ê¸°ì¤€ ë¶„ë¥˜ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!") 