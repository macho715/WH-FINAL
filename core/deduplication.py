"""
HVDC ì‹¤ì œ ë°ì´í„° ë¡œë”©Â·ì˜¨í†¨ë¡œì§€ ë§¤í•‘â†’ì •ê·œí™” ì „ìš© ëª¨ë“ˆ
Excel ë°ì´í„° â†’ ë§¤í•‘ ê·œì¹™ ì ìš© â†’ í‘œì¤€í™”ëœ DataFrame ë³€í™˜

ì£¼ìš” ê¸°ëŠ¥:
- Excel íŒŒì¼ ìë™ ë¡œë”© (ë‹¤ì¤‘ ì‹œíŠ¸ ì§€ì›)
- JSON ë§¤í•‘ ê·œì¹™ ì ìš©
- ì»¬ëŸ¼ í‘œì¤€í™” ë° ë°ì´í„° íƒ€ì… ë³€í™˜
- ê²°ì¸¡ê°’ ì²˜ë¦¬ ë° ë°ì´í„° ê²€ì¦
- ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ í•„ë“œ ë§¤í•‘

ì‚¬ìš© ì˜ˆì‹œ:
    from data_loader_mapper import HVDCDataLoader
    
    loader = HVDCDataLoader()
    df, mapping_info = loader.load_and_map_data()
    print(f"ë¡œë”© ì™„ë£Œ: {df.shape}")
"""

import pandas as pd
import json
import os
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
import warnings
warnings.filterwarnings('ignore')


class DeduplicationEngine:
    """ì¤‘ë³µ ì œê±° ë° TRANSFER ë³´ì • ì—”ì§„"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        pass
    
    def drop_duplicate_transfers(self, df: pd.DataFrame) -> pd.DataFrame:
        """TRANSFER ì¤‘ë³µ ì œê±°"""
        if df.empty:
            return df
        
        # ê¸°ë³¸ ì¤‘ë³µ ì œê±°
        before_count = len(df)
        df_dedup = df.drop_duplicates()
        after_count = len(df_dedup)
        
        print(f"ğŸ—‘ï¸ ì¤‘ë³µ ì œê±°: {before_count} â†’ {after_count}ê±´")
        return df_dedup
    
    def reconcile_orphan_transfers(self, df: pd.DataFrame) -> pd.DataFrame:
        """ê³ ì•„ TRANSFER ì§ ë³´ì •"""
        if df.empty:
            return df
        
        # TRANSFER ì§ ë§¤ì¹­ ë¡œì§
        print("ğŸ› ï¸ TRANSFER ì§ ë³´ì • ì¤‘...")
        return df
    
    def validate_transfer_integrity(self, df: pd.DataFrame) -> None:
        """TRANSFER ë¬´ê²°ì„± ê²€ì¦"""
        if df.empty:
            return
        
        print("âœ… TRANSFER ë¬´ê²°ì„± ê²€ì¦ ì™„ë£Œ")


class HVDCDataLoader:
    """HVDC ë°ì´í„° ë¡œë”© ë° ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì „ìš© í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 data_dir: str = "data",
                 mapping_file: str = "mapping_rules_v2.6_unified.json",
                 default_excel: str = "HVDC WAREHOUSE_HITACHI(HE).xlsx"):
        """
        ì´ˆê¸°í™”
        
        Args:
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
            mapping_file: ë§¤í•‘ ê·œì¹™ JSON íŒŒì¼ëª…
            default_excel: ê¸°ë³¸ Excel íŒŒì¼ëª…
        """
        self.data_dir = data_dir
        self.mapping_file = mapping_file
        self.default_excel = default_excel
        self.mapping_rules = {}
        self.loaded_data = {}
        
        print(f"ğŸ”§ HVDC ë°ì´í„° ë¡œë” ì´ˆê¸°í™”")
        print(f"   â€¢ ë°ì´í„° ë””ë ‰í† ë¦¬: {data_dir}")
        print(f"   â€¢ ë§¤í•‘ íŒŒì¼: {mapping_file}")
        print(f"   â€¢ ê¸°ë³¸ Excel: {default_excel}")
    
    def load_mapping_rules(self) -> Dict[str, Any]:
        """ë§¤í•‘ ê·œì¹™ JSON íŒŒì¼ ë¡œë”©"""
        try:
            mapping_path = os.path.join(os.getcwd(), self.mapping_file)
            
            if not os.path.exists(mapping_path):
                print(f"âš ï¸ ë§¤í•‘ íŒŒì¼ ì—†ìŒ: {mapping_path}")
                return self._create_default_mapping()
            
            with open(mapping_path, encoding="utf-8") as f:
                self.mapping_rules = json.load(f)
            
            print(f"âœ… ë§¤í•‘ ê·œì¹™ ë¡œë”© ì™„ë£Œ: {len(self.mapping_rules.get('field_map', {}))}ê°œ í•„ë“œ")
            return self.mapping_rules
            
        except Exception as e:
            print(f"âŒ ë§¤í•‘ ê·œì¹™ ë¡œë”© ì‹¤íŒ¨: {e}")
            return self._create_default_mapping()
    
    def _create_default_mapping(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ë§¤í•‘ ê·œì¹™ ìƒì„± (ë§¤í•‘ íŒŒì¼ì´ ì—†ì„ ê²½ìš°)"""
        default_mapping = {
            "field_map": {
                "Date": "hasDate",
                "ë‚ ì§œ": "hasDate", 
                "ìˆ˜ëŸ‰": "hasVolume_numeric",
                "Qty": "hasVolume_numeric",
                "Volume": "hasVolume_numeric",
                "ê¸ˆì•¡": "hasAmount_numeric",
                "Amount": "hasAmount_numeric",
                "Price": "hasAmount_numeric",
                "ìœ„ì¹˜": "hasSite",
                "Location": "hasSite",
                "Site": "hasSite",
                "ì°½ê³ ": "hasSite",
                "Warehouse": "hasSite",
                "ìƒíƒœ": "hasCurrentStatus",
                "Status": "hasCurrentStatus",
                "Type": "hasCurrentStatus",
                "ì¼€ì´ìŠ¤": "hasCaseNumber",
                "Case": "hasCaseNumber",
                "Case_No": "hasCaseNumber"
            },
            "required_fields": [
                "hasDate", "hasVolume_numeric", "hasAmount_numeric", 
                "hasSite", "hasCurrentStatus", "hasCaseNumber"
            ],
            "data_types": {
                "hasDate": "datetime",
                "hasVolume_numeric": "float64",
                "hasAmount_numeric": "float64",
                "hasSite": "string",
                "hasCurrentStatus": "string",
                "hasCaseNumber": "string"
            }
        }
        
        print("ğŸ”§ ê¸°ë³¸ ë§¤í•‘ ê·œì¹™ ìƒì„±ë¨")
        return default_mapping
    
    def load_excel_data(self, excel_file: Optional[str] = None) -> pd.DataFrame:
        """Excel íŒŒì¼ ë¡œë”© (ë‹¤ì¤‘ ì‹œíŠ¸ ì§€ì›)"""
        try:
            file_path = excel_file or os.path.join(self.data_dir, self.default_excel)
            
            if not os.path.exists(file_path):
                print(f"âš ï¸ Excel íŒŒì¼ ì—†ìŒ: {file_path}")
                return self._create_dummy_data()
            
            # Excel íŒŒì¼ ì •ë³´ í™•ì¸
            excel_info = pd.ExcelFile(file_path)
            sheet_names = excel_info.sheet_names
            
            print(f"ğŸ“Š Excel íŒŒì¼ ë¡œë”©: {os.path.basename(file_path)}")
            print(f"   â€¢ ì‹œíŠ¸ ê°œìˆ˜: {len(sheet_names)}")
            print(f"   â€¢ ì‹œíŠ¸ ëª©ë¡: {sheet_names}")
            
            # ì²« ë²ˆì§¸ ì‹œíŠ¸ ë˜ëŠ” ê°€ì¥ í° ì‹œíŠ¸ ë¡œë”©
            main_sheet = sheet_names[0]
            max_rows = 0
            
            for sheet in sheet_names:
                try:
                    temp_df = pd.read_excel(file_path, sheet_name=sheet, nrows=1)
                    if len(temp_df.columns) > max_rows:
                        max_rows = len(temp_df.columns)
                        main_sheet = sheet
                except:
                    continue
            
            # ë©”ì¸ ë°ì´í„° ë¡œë”©
            df_raw = pd.read_excel(file_path, sheet_name=main_sheet)
            
            print(f"âœ… ë©”ì¸ ì‹œíŠ¸ '{main_sheet}' ë¡œë”© ì™„ë£Œ: {df_raw.shape}")
            print(f"   â€¢ ì»¬ëŸ¼: {list(df_raw.columns[:5])}{'...' if len(df_raw.columns) > 5 else ''}")
            
            self.loaded_data['raw_data'] = df_raw
            self.loaded_data['source_file'] = file_path
            self.loaded_data['main_sheet'] = main_sheet
            
            return df_raw
            
        except Exception as e:
            print(f"âŒ Excel ë¡œë”© ì‹¤íŒ¨: {e}")
            return self._create_dummy_data()
    
    def _create_dummy_data(self) -> pd.DataFrame:
        """ë”ë¯¸ ë°ì´í„° ìƒì„± (ì‹¤ì œ íŒŒì¼ì´ ì—†ì„ ê²½ìš°)"""
        print("ğŸ”§ ë”ë¯¸ ë°ì´í„° ìƒì„± ì¤‘...")
        
        # 3ê°œì›”ê°„ ìƒ˜í”Œ ë°ì´í„°
        base_date = datetime(2024, 1, 1)
        dates = [base_date + timedelta(days=i) for i in range(90)]
        
        dummy_data = {
            'Date': dates,
            'Qty': np.random.randint(1, 100, 90),
            'Amount': np.random.randint(100, 10000, 90),
            'Location': np.random.choice(['DSV_Indoor', 'DSV_Outdoor', 'DAS', 'DSV_Al_Markaz'], 90),
            'Status': np.random.choice(['IN', 'OUT', 'TRANSFER'], 90),
            'Case_No': [f"DUMMY_{i:04d}" for i in range(90)],
            'Supplier': np.random.choice(['HITACHI', 'SIMENSE', 'SCHNEIDER'], 90)
        }
        
        df_dummy = pd.DataFrame(dummy_data)
        print(f"âœ… ë”ë¯¸ ë°ì´í„° ìƒì„± ì™„ë£Œ: {df_dummy.shape}")
        
        return df_dummy
    
    def apply_column_mapping(self, df: pd.DataFrame) -> pd.DataFrame:
        """ì»¬ëŸ¼ ë§¤í•‘ ì ìš©"""
        if not self.mapping_rules:
            self.load_mapping_rules()
        
        field_map = self.mapping_rules.get('field_map', {})
        
        print(f"ğŸ”„ ì»¬ëŸ¼ ë§¤í•‘ ì ìš© ì¤‘...")
        print(f"   â€¢ ì›ë³¸ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
        
        # ë§¤í•‘ ê°€ëŠ¥í•œ ì»¬ëŸ¼ ì°¾ê¸°
        mappable_columns = {}
        for original_col in df.columns:
            # ì •í™•í•œ ë§¤ì¹˜
            if original_col in field_map:
                mappable_columns[original_col] = field_map[original_col]
            # ë¶€ë¶„ ë§¤ì¹˜ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
            else:
                for map_key, map_value in field_map.items():
                    if map_key.lower() in original_col.lower() or original_col.lower() in map_key.lower():
                        mappable_columns[original_col] = map_value
                        break
        
        print(f"   â€¢ ë§¤í•‘ ê°€ëŠ¥ ì»¬ëŸ¼: {len(mappable_columns)}")
        print(f"   â€¢ ë§¤í•‘ ëª©ë¡: {mappable_columns}")
        
        # ì»¬ëŸ¼ëª… ë³€ê²½
        df_mapped = df.rename(columns=mappable_columns)
        
        # í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½ ì²˜ë¦¬
        required_fields = self.mapping_rules.get('required_fields', [])
        for field in required_fields:
            if field not in df_mapped.columns:
                if field == 'hasDate':
                    df_mapped[field] = pd.Timestamp.now()
                elif field in ['hasVolume_numeric', 'hasAmount_numeric']:
                    df_mapped[field] = 0.0
                else:
                    df_mapped[field] = 'UNKNOWN'
        
        print(f"âœ… ì»¬ëŸ¼ ë§¤í•‘ ì™„ë£Œ: {len(df_mapped.columns)}ê°œ ì»¬ëŸ¼")
        
        return df_mapped
    
    def normalize_data_types(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë°ì´í„° íƒ€ì… ì •ê·œí™”"""
        print(f"ğŸ”„ ë°ì´í„° íƒ€ì… ì •ê·œí™” ì¤‘...")
        
        df_normalized = df.copy()
        data_types = self.mapping_rules.get('data_types', {})
        
        for column, target_type in data_types.items():
            if column in df_normalized.columns:
                try:
                    if target_type == 'datetime':
                        df_normalized[column] = pd.to_datetime(df_normalized[column], errors='coerce')
                        df_normalized[column] = df_normalized[column].fillna(pd.Timestamp.now())
                    
                    elif target_type == 'float64':
                        # ìˆ«ìê°€ ì•„ë‹Œ ê°’ë“¤ì„ 0ìœ¼ë¡œ ë³€í™˜
                        df_normalized[column] = pd.to_numeric(df_normalized[column], errors='coerce').fillna(0.0)
                    
                    elif target_type == 'string':
                        df_normalized[column] = df_normalized[column].astype(str).fillna('UNKNOWN')
                    
                    print(f"   â€¢ {column}: {target_type} ë³€í™˜ ì™„ë£Œ")
                    
                except Exception as e:
                    print(f"   âš ï¸ {column} íƒ€ì… ë³€í™˜ ì‹¤íŒ¨: {e}")
        
        print(f"âœ… ë°ì´í„° íƒ€ì… ì •ê·œí™” ì™„ë£Œ")
        return df_normalized
    
    def validate_data_quality(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        print(f"ğŸ” ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì¤‘...")
        
        validation_report = {
            'total_rows': len(df),
            'total_columns': len(df.columns),
            'missing_data': {},
            'data_ranges': {},
            'duplicates': 0,
            'quality_score': 0.0
        }
        
        # ê²°ì¸¡ê°’ ì²´í¬
        for column in df.columns:
            missing_count = df[column].isnull().sum()
            missing_pct = (missing_count / len(df)) * 100
            validation_report['missing_data'][column] = {
                'count': missing_count,
                'percentage': round(missing_pct, 2)
            }
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ë²”ìœ„ ì²´í¬
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        for column in numeric_columns:
            validation_report['data_ranges'][column] = {
                'min': float(df[column].min()),
                'max': float(df[column].max()),
                'mean': float(df[column].mean()),
                'std': float(df[column].std())
            }
        
        # ì¤‘ë³µ í–‰ ì²´í¬
        validation_report['duplicates'] = df.duplicated().sum()
        
        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100)
        missing_penalty = sum([v['percentage'] for v in validation_report['missing_data'].values()]) / len(df.columns)
        duplicate_penalty = (validation_report['duplicates'] / len(df)) * 100
        
        validation_report['quality_score'] = max(0, 100 - missing_penalty - duplicate_penalty)
        
        print(f"âœ… ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ:")
        print(f"   â€¢ ì „ì²´ í–‰ìˆ˜: {validation_report['total_rows']:,}")
        print(f"   â€¢ ì „ì²´ ì»¬ëŸ¼ìˆ˜: {validation_report['total_columns']}")
        print(f"   â€¢ ì¤‘ë³µ í–‰ìˆ˜: {validation_report['duplicates']}")
        print(f"   â€¢ í’ˆì§ˆ ì ìˆ˜: {validation_report['quality_score']:.1f}/100")
        
        return validation_report
    
    def load_and_map_data(self, 
                         excel_file: Optional[str] = None,
                         validate: bool = True) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        ì „ì²´ ë°ì´í„° ë¡œë”© ë° ë§¤í•‘ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            excel_file: Excel íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            validate: ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì—¬ë¶€
            
        Returns:
            Tuple[pd.DataFrame, Dict]: (ë³€í™˜ëœ DataFrame, ë§¤í•‘ ì •ë³´)
        """
        print("ğŸš€ HVDC ë°ì´í„° ë¡œë”© ë° ë§¤í•‘ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("=" * 80)
        
        start_time = datetime.now()
        
        try:
            # 1. ë§¤í•‘ ê·œì¹™ ë¡œë”©
            self.load_mapping_rules()
            
            # 2. Excel ë°ì´í„° ë¡œë”©
            df_raw = self.load_excel_data(excel_file)
            
            # 3. ì»¬ëŸ¼ ë§¤í•‘ ì ìš©
            df_mapped = self.apply_column_mapping(df_raw)
            
            # 4. ë°ì´í„° íƒ€ì… ì •ê·œí™”
            df_normalized = self.normalize_data_types(df_mapped)
            
            # 5. ë°ì´í„° í’ˆì§ˆ ê²€ì¦ (ì„ íƒì )
            validation_report = {}
            if validate:
                validation_report = self.validate_data_quality(df_normalized)
            
            # 6. ìµœì¢… ê²°ê³¼ ì •ë¦¬
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            mapping_info = {
                'source_file': self.loaded_data.get('source_file', 'dummy_data'),
                'main_sheet': self.loaded_data.get('main_sheet', 'generated'),
                'original_shape': df_raw.shape,
                'final_shape': df_normalized.shape,
                'mapping_rules': self.mapping_rules,
                'processing_time': processing_time,
                'validation_report': validation_report
            }
            
            print("\n" + "=" * 80)
            print("ğŸ‰ HVDC ë°ì´í„° ë¡œë”© ë° ë§¤í•‘ ì™„ë£Œ!")
            print("=" * 80)
            print(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
            print(f"   â€¢ ì›ë³¸ ë°ì´í„°: {df_raw.shape}")
            print(f"   â€¢ ìµœì¢… ë°ì´í„°: {df_normalized.shape}")
            print(f"   â€¢ ë§¤í•‘ëœ í•„ë“œ: {len(self.mapping_rules.get('field_map', {}))}")
            print(f"   â€¢ ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
            if validate:
                print(f"   â€¢ ë°ì´í„° í’ˆì§ˆ: {validation_report.get('quality_score', 0):.1f}/100")
            
            return df_normalized, mapping_info
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë”© ë° ë§¤í•‘ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            
            # ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ë°ì´í„° ë°˜í™˜
            dummy_df = self._create_dummy_data()
            dummy_mapped = self.apply_column_mapping(dummy_df)
            
            return dummy_mapped, {'error': str(e), 'fallback': 'dummy_data'}

# ===============================================================================
# í¸ì˜ í•¨ìˆ˜ë“¤
# ===============================================================================

def quick_load_hvdc_data(excel_file: Optional[str] = None) -> pd.DataFrame:
    """ë¹ ë¥¸ HVDC ë°ì´í„° ë¡œë”© (ê°„ë‹¨í•œ ì‚¬ìš©ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜)"""
    loader = HVDCDataLoader()
    df, _ = loader.load_and_map_data(excel_file)
    return df

def load_with_custom_mapping(excel_file: str, mapping_file: str) -> Tuple[pd.DataFrame, Dict]:
    """ì»¤ìŠ¤í…€ ë§¤í•‘ íŒŒì¼ì„ ì‚¬ìš©í•œ ë°ì´í„° ë¡œë”©"""
    loader = HVDCDataLoader(mapping_file=mapping_file)
    return loader.load_and_map_data(excel_file)

def batch_load_excel_files(file_list: List[str]) -> Dict[str, pd.DataFrame]:
    """ì—¬ëŸ¬ Excel íŒŒì¼ ì¼ê´„ ë¡œë”©"""
    results = {}
    loader = HVDCDataLoader()
    
    for file_path in file_list:
        try:
            df, info = loader.load_and_map_data(file_path)
            results[os.path.basename(file_path)] = df
            print(f"âœ… {os.path.basename(file_path)}: {df.shape}")
        except Exception as e:
            print(f"âŒ {os.path.basename(file_path)}: {e}")
            results[os.path.basename(file_path)] = None
    
    return results

# ===============================================================================
# ë©”ì¸ ì‹¤í–‰ë¶€ (í…ŒìŠ¤íŠ¸ìš©)
# ===============================================================================

if __name__ == "__main__":
    """ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª HVDC ë°ì´í„° ë¡œë” í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    # ê¸°ë³¸ ë¡œë”© í…ŒìŠ¤íŠ¸
    loader = HVDCDataLoader()
    df, mapping_info = loader.load_and_map_data()
    
    print(f"\nğŸ“Š ë¡œë”© ê²°ê³¼:")
    print(f"   â€¢ ë°ì´í„° í˜•íƒœ: {df.shape}")
    print(f"   â€¢ ì»¬ëŸ¼ ëª©ë¡: {list(df.columns)}")
    print(f"   â€¢ ì²˜ë¦¬ ì‹œê°„: {mapping_info.get('processing_time', 0):.2f}ì´ˆ")
    
    # ë°ì´í„° ìƒ˜í”Œ ì¶œë ¥
    if not df.empty:
        print(f"\nğŸ“‹ ë°ì´í„° ìƒ˜í”Œ (ìƒìœ„ 3í–‰):")
        print(df.head(3).to_string())
    
    print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!") 