#!/usr/bin/env python3
"""
HVDC ë°ì´í„° ë³€í™˜/ì •ê·œí™” ì—”ì§„
==========================

ë°ì´í„° íƒ€ì… ë³€í™˜, ê²°ì¸¡ì¹˜ ì²˜ë¦¬, ì´ìƒì¹˜ ë³´ì •, ê°’ ì •ê·œí™”ë¥¼ ë‹´ë‹¹
"""

import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, Any, Optional
import logging

logger = logging.getLogger(__name__)

class DataTransformer:
    """ë°ì´í„° ì •ê·œí™”/íƒ€ì…ë³€í™˜/ê²°ì¸¡ì¹˜/ì´ìƒì¹˜ ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Args:
            config: ë³€í™˜ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.config = config or {}
        
    def normalize_types(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë°ì´í„° íƒ€ì… ì •ê·œí™”"""
        logger.info("ğŸ”„ ë°ì´í„° íƒ€ì… ì •ê·œí™” ì‹œì‘...")
        
        df_result = df.copy()
        
        # ë‚ ì§œ ì»¬ëŸ¼ ì •ê·œí™”
        date_columns = ['hasDate', 'ETD/ATD', 'ETA/ATA']
        for col in date_columns:
            if col in df_result.columns:
                df_result[col] = pd.to_datetime(df_result[col], errors='coerce')
                logger.info(f"   â€¢ {col}: datetime ë³€í™˜ ì™„ë£Œ")
        
        # ìˆ˜ì¹˜ ì»¬ëŸ¼ ì •ê·œí™”
        numeric_columns = ['hasVolume', 'hasAmount', 'hasQuantity']
        for col in numeric_columns:
            if col in df_result.columns:
                df_result[f'{col}_numeric'] = pd.to_numeric(df_result[col], errors='coerce')
                logger.info(f"   â€¢ {col}: numeric ë³€í™˜ ì™„ë£Œ")
        
        # ë¬¸ìì—´ ì»¬ëŸ¼ ì •ê·œí™”
        string_columns = ['hasSite', 'hasCurrentStatus', 'hasCaseNumber']
        for col in string_columns:
            if col in df_result.columns:
                df_result[col] = df_result[col].astype(str).str.strip()
                logger.info(f"   â€¢ {col}: string ì •ê·œí™” ì™„ë£Œ")
        
        logger.info(f"âœ… ë°ì´í„° íƒ€ì… ì •ê·œí™” ì™„ë£Œ: {df_result.shape}")
        return df_result
    
    def handle_missing(self, df: pd.DataFrame) -> pd.DataFrame:
        """ê²°ì¸¡ì¹˜ ì²˜ë¦¬"""
        logger.info("ğŸ”„ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì‹œì‘...")
        
        df_result = df.copy()
        
        # ë‚ ì§œ ê²°ì¸¡ì¹˜ â†’ í˜„ì¬ ì‹œê°„
        date_columns = ['hasDate']
        for col in date_columns:
            if col in df_result.columns:
                missing_count = df_result[col].isna().sum()
                if missing_count > 0:
                    df_result[col] = df_result[col].fillna(pd.Timestamp.now())
                    logger.info(f"   â€¢ {col}: {missing_count}ê°œ ê²°ì¸¡ì¹˜ â†’ í˜„ì¬ì‹œê°„")
        
        # ìˆ˜ì¹˜ ê²°ì¸¡ì¹˜ â†’ 0
        numeric_columns = [col for col in df_result.columns if col.endswith('_numeric')]
        for col in numeric_columns:
            missing_count = df_result[col].isna().sum()
            if missing_count > 0:
                df_result[col] = df_result[col].fillna(0)
                logger.info(f"   â€¢ {col}: {missing_count}ê°œ ê²°ì¸¡ì¹˜ â†’ 0")
        
        # ë¬¸ìì—´ ê²°ì¸¡ì¹˜ â†’ 'UNKNOWN'
        string_columns = ['hasSite', 'hasCurrentStatus', 'hasCaseNumber']
        for col in string_columns:
            if col in df_result.columns:
                missing_count = df_result[col].isna().sum()
                if missing_count > 0:
                    df_result[col] = df_result[col].fillna('UNKNOWN')
                    logger.info(f"   â€¢ {col}: {missing_count}ê°œ ê²°ì¸¡ì¹˜ â†’ UNKNOWN")
        
        logger.info(f"âœ… ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {df_result.shape}")
        return df_result
    
    def fix_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        """ì´ìƒì¹˜ ë³´ì •"""
        logger.info("ğŸ”„ ì´ìƒì¹˜ ë³´ì • ì‹œì‘...")
        
        df_result = df.copy()
        
        # ìˆ˜ëŸ‰/ê¸ˆì•¡ ìŒìˆ˜ê°’ ë³´ì •
        numeric_columns = [col for col in df_result.columns if col.endswith('_numeric')]
        for col in numeric_columns:
            if col in df_result.columns:
                negative_count = (df_result[col] < 0).sum()
                if negative_count > 0:
                    df_result[col] = df_result[col].abs()
                    logger.info(f"   â€¢ {col}: {negative_count}ê°œ ìŒìˆ˜ê°’ â†’ ì ˆëŒ“ê°’")
        
        # ê·¹ë‹¨ì  ì´ìƒì¹˜ ì œê±° (IQR ë°©ë²•)
        for col in numeric_columns:
            if col in df_result.columns and df_result[col].std() > 0:
                Q1 = df_result[col].quantile(0.25)
                Q3 = df_result[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outlier_count = ((df_result[col] < lower_bound) | (df_result[col] > upper_bound)).sum()
                if outlier_count > 0:
                    df_result[col] = df_result[col].clip(lower=lower_bound, upper=upper_bound)
                    logger.info(f"   â€¢ {col}: {outlier_count}ê°œ ì´ìƒì¹˜ ë³´ì •")
        
        logger.info(f"âœ… ì´ìƒì¹˜ ë³´ì • ì™„ë£Œ: {df_result.shape}")
        return df_result
    
    def normalize_locations(self, df: pd.DataFrame) -> pd.DataFrame:
        """ìœ„ì¹˜ëª… ì •ê·œí™” (ëŒ€ì†Œë¬¸ì, ê³µë°± ë“±)"""
        logger.info("ğŸ”„ ìœ„ì¹˜ëª… ì •ê·œí™” ì‹œì‘...")
        
        df_result = df.copy()
        
        if 'hasSite' in df_result.columns:
            # ëŒ€ì†Œë¬¸ì í†µì¼ ë° ê³µë°± ì œê±°
            df_result['hasSite_normalized'] = (
                df_result['hasSite']
                .str.strip()
                .str.upper()
                .replace('', 'UNKNOWN')
            )
            
            # ì¼ë°˜ì ì¸ ë³€í˜• í†µì¼
            location_mapping = {
                'DAS': 'DAS',
                'Das': 'DAS',
                'das': 'DAS',
                'D.A.S': 'DAS',
                'D A S': 'DAS'
            }
            
            df_result['hasSite_normalized'] = df_result['hasSite_normalized'].replace(location_mapping)
            
            unique_locations = df_result['hasSite_normalized'].value_counts()
            logger.info(f"   â€¢ ì •ê·œí™”ëœ ìœ„ì¹˜: {len(unique_locations)}ê°œ")
            for loc, count in unique_locations.head(10).items():
                logger.info(f"     - {loc}: {count}ê±´")
        
        logger.info(f"âœ… ìœ„ì¹˜ëª… ì •ê·œí™” ì™„ë£Œ: {df_result.shape}")
        return df_result
    
    def run_all(self, df: pd.DataFrame) -> pd.DataFrame:
        """ëª¨ë“  ë³€í™˜ ì‘ì—…ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰"""
        logger.info("ğŸš€ ì „ì²´ ë°ì´í„° ë³€í™˜ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        logger.info("=" * 60)
        
        start_time = datetime.now()
        
        # 1. íƒ€ì… ì •ê·œí™”
        df_result = self.normalize_types(df)
        
        # 2. ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        df_result = self.handle_missing(df_result)
        
        # 3. ì´ìƒì¹˜ ë³´ì •
        df_result = self.fix_outliers(df_result)
        
        # 4. ìœ„ì¹˜ëª… ì •ê·œí™”
        df_result = self.normalize_locations(df_result)
        
        # 5. ìµœì¢… ê²€ì¦
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        logger.info("=" * 60)
        logger.info("ğŸ‰ ì „ì²´ ë°ì´í„° ë³€í™˜ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
        logger.info(f"   â€¢ ì›ë³¸: {df.shape} â†’ ê²°ê³¼: {df_result.shape}")
        logger.info(f"   â€¢ ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
        logger.info(f"   â€¢ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)} â†’ {len(df_result.columns)}")
        
        return df_result
    
    def validate_transformation(self, df_original: pd.DataFrame, df_transformed: pd.DataFrame) -> Dict[str, Any]:
        """ë³€í™˜ ê²°ê³¼ ê²€ì¦"""
        validation_result = {
            "original_shape": df_original.shape,
            "transformed_shape": df_transformed.shape,
            "data_loss": len(df_original) - len(df_transformed),
            "new_columns": list(set(df_transformed.columns) - set(df_original.columns)),
            "missing_data": {},
            "data_types": {}
        }
        
        # ê²°ì¸¡ì¹˜ ê²€ì‚¬
        for col in df_transformed.columns:
            missing_count = df_transformed[col].isna().sum()
            if missing_count > 0:
                validation_result["missing_data"][col] = missing_count
        
        # ë°ì´í„° íƒ€ì… ê²€ì‚¬
        for col in df_transformed.columns:
            validation_result["data_types"][col] = str(df_transformed[col].dtype)
        
        logger.info(f"ğŸ“‹ ë³€í™˜ ê²€ì¦ ê²°ê³¼: {validation_result}")
        
        return validation_result

# í¸ì˜ í•¨ìˆ˜ë“¤
def quick_transform(df: pd.DataFrame) -> pd.DataFrame:
    """ë¹ ë¥¸ ë³€í™˜ (ê¸°ë³¸ ì„¤ì •)"""
    transformer = DataTransformer()
    return transformer.run_all(df)

def validate_data_quality(df: pd.DataFrame) -> Dict[str, Any]:
    """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
    quality_report = {
        "total_records": len(df),
        "total_columns": len(df.columns),
        "missing_data_summary": {},
        "data_type_summary": {},
        "duplicates": df.duplicated().sum(),
        "memory_usage": df.memory_usage(deep=True).sum() / 1024 / 1024  # MB
    }
    
    # ê²°ì¸¡ì¹˜ ìš”ì•½
    for col in df.columns:
        missing_count = df[col].isna().sum()
        missing_pct = (missing_count / len(df)) * 100
        if missing_count > 0:
            quality_report["missing_data_summary"][col] = {
                "count": missing_count,
                "percentage": round(missing_pct, 2)
            }
    
    # ë°ì´í„° íƒ€ì… ìš”ì•½
    type_counts = df.dtypes.value_counts()
    quality_report["data_type_summary"] = type_counts.to_dict()
    
    return quality_report

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("ğŸ§ª DataTransformer í…ŒìŠ¤íŠ¸")
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    sample_data = {
        'hasSite': ['DAS', 'AGI', 'MIR', 'Das', None],
        'hasVolume': ['100.5', '200', '-50', 'invalid', '300'],
        'hasAmount': [1000, 2000, -500, None, 3000],
        'hasDate': ['2024-01-01', '2024-01-02', None, '2024-01-04', '2024-01-05']
    }
    
    df_test = pd.DataFrame(sample_data)
    print(f"ì›ë³¸ ë°ì´í„°:\n{df_test}")
    
    # ë³€í™˜ ì‹¤í–‰
    transformer = DataTransformer()
    df_transformed = transformer.run_all(df_test)
    print(f"\në³€í™˜ëœ ë°ì´í„°:\n{df_transformed}")
    
    # í’ˆì§ˆ ê²€ì¦
    quality_report = validate_data_quality(df_transformed)
    print(f"\në°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸:\n{quality_report}") 